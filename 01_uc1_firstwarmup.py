# -*- coding: utf-8 -*-
"""01_UC1_FirstWarmUp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PmUmoXuNGYZKqaRGLOgblq0pAXyfvSXG

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-0/basics.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/56295530-getting-set-up-video-guide)

# AIChampionsHub : Academy

### Module 1: Interacting with Large Language Models

### Use Case 1A : Warm-up - LLMs using Langchain
This is part of Course by **AIChampionsHub** - AI Fundamentals and AI Engineering Courses leverage this Notebook.

---
<a href="https://github.com/aichampionslearn/01_LLM_Basics"><img src="https://img.shields.io/badge/GitHub%20Repository-black?logo=github"></a>
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/aichampionslearn/01_LLM_Basics/blob/main/AICH_L2_AIAgents_M1_D3_BasicLLMAppv01.ipynb)

### ChatModels

- In simple terms a Chat Model takes a sequences of messages as Inputs and returns response (or Chat messages) as output(s).
- Langchain Primarily integrates with multiple third-parties (LLM Providers or Frameworks).
- In this initial lesson we will use OpenAI's [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) and [ChatOllama](https://python.langchain.com/v0.2/docs/integrations/chat/ollama/)

For OpenAI please make sure that you a `OPENAI_API_KEY`.
"""
import os, getpass
# from google.colab import userdata

def _set_OpenAIKey(var: str, env:int):
    key = os.environ[var]
    # print(key)
    return key;

OPENAI_API_KEY = _set_OpenAIKey("OPENAI_API_KEY",0) #0 for reading from userdata


"""## Key Points
- Instantiate we can instantiate our `ChatOpenAI` model object. Specify the model from series offered e.g. `gpt-4o` or `gpt-3.5`
- Set Parameters as required

  * `model`: the name of the model
  * `temperature`: the sampling temperature

- Note: `Temperature` closer to 0 (low) is more deterministic and Closer 1 implies user is okay with model generating varied responses or being creative.
"""

print("\n Method 01 - Using Open AI \n\n")
from langchain_openai import ChatOpenAI
MODEL =  "gpt-3.5-turbo-0125" # or "gpt-4o"
llm = ChatOpenAI(model= MODEL, temperature=0)

"""- `invoke` method can be used to call the chain on an input. In `messages`, do specify a role and content property."""

from langchain_core.messages import HumanMessage

# Create a message
messages = HumanMessage(content="What is the Capital of India?", name="AIChampion")

# Invoke the model with a LIST of messages
response = llm.invoke([messages])
# print(response) # This will print the dictionary of response attribtues
print(response.content)

"""## Method 2 : Using ollama - if you are running the code locally.
- See this link for details like downloading model etc..[ChatOllama](https://python.langchain.com/v0.2/docs/integrations/chat/ollama/)

- Note you should run this portion of code or this method in local as python file
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-ollama
print("\n Method 02 - Using Ollama \n\n")

from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.1",
    temperature=0,
    # other params...
)

# from langchain_core.messages import HumanMessage, AIMessage
messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love Artificial Intelligence."),
]
response = llm.invoke(messages)
print(response.content)
